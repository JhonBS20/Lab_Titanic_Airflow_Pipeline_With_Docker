# Laboratorio Titanic Airflow Pipeline con Docker

Este laboratorio se basa en el [video tutorial](https://youtu.be/zkZPBHzvoMA) **Orquestando un Data Pipeline con Apache Airflow**, realizado por Ronny Gago (Data Engineer) a través de un webinar en el canal de Smart Data en Youtube.

## Tecnologías

En este laboratorio se utiliza **Docker Desktop** como herramienta principal para conectarse a Apache Airflow y establecer un workflow con tareas de almacenamiento, transformación y visualización de datos.

Herramientas:

- Docker (Docker Desktop)
- Apache Airflow
- PostgreSQL -> Software DBeaver

## Arquitectura

![Titanic Arquitecture](https://github.com/JhonBS20/Lab_Titanic_Airflow_Pipeline_With_Docker/blob/main/Titanic%20Arquitecture.png?raw=true)
